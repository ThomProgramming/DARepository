{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898c6227",
   "metadata": {},
   "source": [
    "# Aufschrieb ML-Projekt\n",
    "\n",
    "RMSE bevorzugt, aber sensitiv ggü. Outliers\n",
    "MAE weniger sensitiv ggü. Outliers\n",
    "\n",
    "Ergebnis bekannt: daher supervised learning\n",
    "Nutzung von mehreren features zur Zielberechnung: multiple regression\n",
    "Vorhersage nur eines Wertes: univariate regression (ansonsten multivariate r.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d89748-8e04-4b64-a877-6f899f2bbed5",
   "metadata": {},
   "source": [
    "# Prüfung\n",
    "\n",
    "Notebook lineare Regression (mit BMI)  \n",
    "y = mx+t  \n",
    "b0 = t  \n",
    "\n",
    "\n",
    "Art des Zusammenhangs: positive Korrelation, negative Korrelation, keine\n",
    "Linearer Zusammenhang oder nicht?\n",
    "Streuung?\n",
    "Welche Features kann man noch einbauen/Parameter können noch betrachtet werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd01e88",
   "metadata": {},
   "source": [
    "# Aufschriebe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80368c",
   "metadata": {},
   "source": [
    "## Grundlagen\n",
    "\n",
    "Confidence Interval = Bereich, in dem sich mit 95%iger Wahrscheinlichkeit 95% aller Werte befinden. Auffindbar über die Verdopplung der Standardabweichung\n",
    "\n",
    "## Ablauf\n",
    "\n",
    "1. Look at the big picture.\n",
    "\n",
    "Zielsetzung des Projektes, was sollen die Daten abbilden, ist die Zielsetzung umsetzbar?\n",
    "Definieren der Zielvariable\n",
    "\n",
    "2. Get the data.\n",
    "\n",
    "Daten downloaden. Stratified sampling: representative Daten aufbauen, sodass keine bias entsteht. Aufteilen in Train und Test set (train_test_split)\n",
    "\n",
    "3. Explore and visualize the data to gain insights.\n",
    "\n",
    "Korrelationen betrachten (falls möglich), Unreinheiten/Outliers entfernen (falls möglich)\n",
    "Neue Attribute schaffen (falls sinnvoll)\n",
    "\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "\n",
    "Fehlende Werte ersetzen/Attribut löschen/Reihe löschen\n",
    "Klategorische Attribute aus nominalen Attributen bilden (one-hot-encoder falls Werte nicht automatisch in richtiger Reihenfolge [1 näher an 2 als an 4 = richtig])\n",
    "Skalierungen anpassen (Standardization oder min-max-scaling, bei heavy tail [Verteilung übermäßig in einer Richtung] entweder Wurzel des Features oder Log oder bucketizing)\n",
    "\n",
    "Feature Transformation:\n",
    "\n",
    "\n",
    "    have missing values\n",
    "\n",
    "    contain a small number of extreme values (outliers)\n",
    "\n",
    "    be on vastly different scales\n",
    "\n",
    "    need to be numeric instead of categorical\n",
    "\n",
    "    follow a skewed distribution where a small proportion of samples are orders of magnitude larger than the majority of the data (i.e., skewness).\n",
    "\n",
    "    be censored on the low and/or high end of the range.\n",
    "\n",
    "\n",
    "5. Select a model and train it.\n",
    "\n",
    "Underfitting -> großer absoluter Fehler im Vergleich zu Zielgröße\n",
    "Overfitting -> kleiner/kein Fehler\n",
    "\n",
    "6. Fine-tune your model.\n",
    "\n",
    "Cross-Validation: bspw. k-Folds, multiples Training/Validating\n",
    "Overfitting -> kleiner/kein Fehler beim Training set, hoher Fehler bei Validation\n",
    "Gridsearch: automatisches tuning von Hyperparametern durch 'Ausprobieren' nach input, welche Parameter probiert werden sollen -> rechenintensiv!\n",
    "Randomsearch -> weniger rechenintensiv\n",
    "feature importance analysieren und aussortieren\n",
    "\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf30c4d-c68a-4e04-83cc-3d81228c1bb4",
   "metadata": {},
   "source": [
    "## Lineare Regression (supervised)\n",
    "\n",
    "x = unabh.\n",
    "y = Abh.\n",
    "\n",
    "Methode der kleinsten Quadrate: Abstand der Punkte zur Regressionslinie. Quadrierung um eine Nullung des Fehlers zu vermeiden -> Fehler heben sich ansonsten auf! (RMSE)\n",
    "\n",
    "Homoskedastizität (H0) \n",
    "Heteroskedastizität: Modellierung der Fehler im Graph; Streuung kann sich unterscheiden je nach Größe der Werte\n",
    "\n",
    "Irrtumswahrscheinlichkeit < 0,05 -> statistische Relevanz gegeben (p<0,05)\n",
    "\n",
    "In welchem Wertebereich macht die Modellierung Sinn/in welchem nicht?\n",
    "\n",
    "R2 = bei 100% keine Streuung, je niedriger desto höher die Streuung \n",
    "f-Wert: zwischen 0 und 1, je größer umso besser sagt das Modell voraus im Vergleich zum Mittelwert. Wert unter 0 -> Modell scheiße"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec288a",
   "metadata": {},
   "source": [
    "## Lineare Regression verbessern\n",
    "\n",
    "### Polynomische Regression\n",
    "\n",
    "Modell wird um exponenzierte Varianten der unabhängigen Variable erweitert. \n",
    "\n",
    "### Step Functions\n",
    "\n",
    "Aufteilung des X-Bereiches in Container (bins). X kann nur in einem Intervall liegen, somit nur zu einem Bereich gehören. Kann ungenau werden falls keine natürlichen Breakpoints vorhanden sind.\n",
    "\n",
    "### Basis Functions\n",
    "\n",
    "Vorherige sind Ausprägungen der Basis Functions; Funktionen, mit denen das Modell angepasst werden kann auf verschiedenen Ebenen. Funktionen sind im Vorhinein bestimmt und somit bekannt.\n",
    "\n",
    "### Regression Splines\n",
    "\n",
    "Oftmals Polynomischer Regression überlegen, Teilen an knots Modell auf, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc408d5",
   "metadata": {},
   "source": [
    "## Klassifikation (supervised)\n",
    "\n",
    "Einordnung in Klassen (ist ein Haus in NY oder in SanFran?) [basierend auf verschiedenen Decision Trees (mehrere Features)]\n",
    "Kategorische Variablen sind Grundlage\n",
    "\n",
    "Für binäre kategorische Variablen ließe sich anhand von dummy variablen auch Regression nutzen, ist aber immer schlechter als Kla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66ef8f",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Outliers: abnormal hohe y-Werte\n",
    "Leverage: abnormal hohe x-Werte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905db34",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "\n",
    "Methode: Teilen des Bereichs der unabhängigen Variable in einfache Regionen, die als Baum dargestellt werden können.\n",
    "Ziel: Varianz und Bias minimieren\n",
    "\n",
    "Nicht so akkurat wie verbesserte Regressionsmethoden, stark interpretierbar\n",
    "Segmentiert das Streudiagramm in Bereiche, die die RSS minimieren (recursive binary splitting)\n",
    "Knoten innerhalb des Baumes: internal nodes\n",
    "Knoten am Ende des Baumes: leaves\n",
    "Verbindung zwischen nodes: branches\n",
    "\n",
    "### Regression Trees\n",
    "\n",
    "Modellierte Antwort ist gegeben durch durchschnittliche Antwort der Trainingsdaten derselben node\n",
    "\n",
    "Overfitting wahrscheinlich, kleiner Baum bringt niedrigere Varianz und bessere Interpretation auf Kosten von bias. \n",
    "\n",
    "Trees filtern nach Methode, RSS um hohen Mindestwert zu reduzieren => pruning (kürzen) um subtrees zu erhalten | Methode: cost complexity pruning (alpha zur Minimierung des durchschnittlichen Fehlers)\n",
    "\n",
    "### Classification Trees\n",
    "\n",
    "Gini-index um node Reinheit zu bestimmen: je niedriger, umso mehr/wahrscheinlicher sind die Ergebnisse aus derselben Klasse\n",
    "Entropie: Ähnlich zu Gini-index, bestimmt ebenfalls Reinheit\n",
    "classification error rate: genutzt wenn accuracy des finalen, gekürtzten trees das Ziel ist.\n",
    "\n",
    "Vorteie von Trees: \n",
    "- einfacher zu verstehen und zu erklären\n",
    "- bilden menschliche Entscheidungsfindung besser ab als klassische Reg und KLa\n",
    "- Können graphisch dargestellt und einfach verstanden werden\n",
    "\n",
    "Nachteile von Trees:\n",
    "- weniger akkurat\n",
    "- können weniger robust sein\n",
    "\n",
    "### Bagging\n",
    "\n",
    "separate Trainingssets schaffen, Mittel daraus bilden -> reduziert Varianz, genauerer Tree\n",
    "große Zahl separater Trainingssets führt nicht zu overfitting!\n",
    "Bäume korrelieren miteinander, da starke Predictors immer zuerst gesplittet werden\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "keine Korrelatin mehr, bessere performance\n",
    "-> Bedeutungsgewichtung von Predictors macht einen Unterschied!\n",
    "\n",
    "### Boosting\n",
    "\n",
    "lernt langsam, indem Baum an Reste anpasst.\n",
    "Baut Bäume aufeinander auf\n",
    "Kann overfitten wenn zu viele Trainingssets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263a9e6",
   "metadata": {},
   "source": [
    "# Clustering (unsupervised)\n",
    "\n",
    "## Hierarchisch\n",
    "\n",
    "Clustering nach Aufteilung, immer nach Distanz (euklidisch[Luftlinie] oder Manhatten), bis ultimative Cluster gebildet sind\n",
    "Verschiedene Ansätze: durchschnittlich kürzeste Distanz pro Pukt, insgesamt kleinster Distanzdurchschnitt\n",
    "\n",
    "## k-means\n",
    "\n",
    "Definieren von k und dann Clustern. Schwerpunkt des Clusters bestimmen, und dann verschieben -> neuer Schwerpunkt, Punkte neu zuordnen nach Distanz zum Schwerpunkt -> Schwerpunkt verschieben -> neuer Schwerpunkt..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c4a69",
   "metadata": {},
   "source": [
    "## Prüfungsfragen\n",
    "\n",
    "Hyperparameteranpassung: Gridsearch, Kombinationscheck \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe505a7",
   "metadata": {},
   "source": [
    "# Python Notizen\n",
    "\n",
    "source activate xgboost\n",
    "```python \n",
    "#virtuelle Umgebung starten (hier xgboost)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
