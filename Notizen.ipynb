{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898c6227",
   "metadata": {},
   "source": [
    "# Aufschrieb ML-Projekt\n",
    "\n",
    "RMSE bevorzugt, aber sensitiv ggü. Outliers\n",
    "MAE weniger sensitiv ggü. Outliers\n",
    "\n",
    "Ergebnis bekannt: daher supervised learning\n",
    "Nutzung von mehreren features zur Zielberechnung: multiple regression\n",
    "Vorhersage nur eines Wertes: univariate regression (ansonsten multivariate r.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d89748-8e04-4b64-a877-6f899f2bbed5",
   "metadata": {},
   "source": [
    "# Prüfung\n",
    "\n",
    "Notebook lineare Regression (mit BMI)  \n",
    "y = mx+t  \n",
    "b0 = t  \n",
    "\n",
    "\n",
    "Art des Zusammenhangs: positive Korrelation, negative Korrelation, keine\n",
    "Linearer Zusammenhang oder nicht?\n",
    "Streuung?\n",
    "Welche Features kann man noch einbauen/Parameter können noch betrachtet werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd01e88",
   "metadata": {},
   "source": [
    "# Aufschriebe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80368c",
   "metadata": {},
   "source": [
    "## Grundlagen\n",
    "\n",
    "Confidence Interval = Bereich, in dem sich mit 95%iger Wahrscheinlichkeit 95% aller Werte befinden. Auffindbar über die Verdopplung der Standardabweichung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf30c4d-c68a-4e04-83cc-3d81228c1bb4",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "\n",
    "x = unabh.\n",
    "y = Abh.\n",
    "\n",
    "Methode der kleinsten Quadrate: Abstand der Punkte zur Regressionslinie. Quadrierung um eine Nullung des Fehlers zu vermeiden -> Fehler heben sich ansonsten auf!\n",
    "\n",
    "Homoskedastizität (H0) \n",
    "Heteroskedastizität: Modellierung der Fehler im Graph; Streuung kann sich unterscheiden je nach Größe der Werte\n",
    "\n",
    "Irrtumswahrscheinlichkeit < 0,05 -> statistische Relevanz gegeben (p<0,05)\n",
    "\n",
    "In welchem Wertebereich macht die Modellierung Sinn/in welchem nicht?\n",
    "\n",
    "R2 = bei 100% keine Streuung, je niedriger desto höher die Streuung \n",
    "f-Wert: zwischen 0 und 1, je größer umso besser sagt das Modell voraus im Vergleich zum Mittelwert. Wert unter 0 -> Modell scheiße"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec288a",
   "metadata": {},
   "source": [
    "## Lineare Regression verbessern\n",
    "\n",
    "### Polynomische Regression\n",
    "\n",
    "Modell wird um exponenzierte Varianten der unabhängigen Variable erweitert. \n",
    "\n",
    "### Step Functions\n",
    "\n",
    "Aufteilung des X-Bereiches in Container (bins). X kann nur in einem Intervall liegen, somit nur zu einem Bereich gehören. Kann ungenau werden falls keine natürlichen Breakpoints vorhanden sind.\n",
    "\n",
    "### Basis Functions\n",
    "\n",
    "Vorherige sind Ausprägungen der Basis Functions; Funktionen, mit denen das Modell angepasst werden kann auf verschiedenen Ebenen. Funktionen sind im Vorhinein bestimmt und somit bekannt.\n",
    "\n",
    "### Regression Splines\n",
    "\n",
    "Oftmals Polynomischer Regression überlegen, Teilen an knots Modell auf, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc408d5",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "\n",
    "Einordnung in Klassen (ist ein Haus in NY oder in SanFran?) [basierend auf verschiedenen Decision Trees (mehrere Features)]\n",
    "Kategorische Variablen sind Grundlage\n",
    "\n",
    "Für binäre kategorische Variablen ließe sich anhand von dummy variablen auch Regression nutzen, ist aber immer schlechter als Kla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66ef8f",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Outliers: abnormal hohe y-Werte\n",
    "Leverage: abnormal hohe x-Werte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905db34",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "\n",
    "Methode: Teilen des Bereichs der unabhängigen Variable in einfache Regionen, die als Baum dargestellt werden können.\n",
    "Ziel: Varianz und Bias minimieren\n",
    "\n",
    "Nicht so akkurat wie verbesserte Regressionsmethoden, stark interpretierbar\n",
    "Segmentiert das Streudiagramm in Bereiche, die die RSS minimieren (recursive binary splitting)\n",
    "Knoten innerhalb des Baumes: internal nodes\n",
    "Knoten am Ende des Baumes: leaves\n",
    "Verbindung zwischen nodes: branches\n",
    "\n",
    "### Regression Trees\n",
    "\n",
    "Modellierte Antwort ist gegeben durch durchschnittliche Antwort der Trainingsdaten derselben node\n",
    "\n",
    "Overfitting wahrscheinlich, kleiner Baum bringt niedrigere Varianz und bessere Interpretation auf Kosten von bias. \n",
    "\n",
    "Trees filtern nach Methode, RSS um hohen Mindestwert zu reduzieren => pruning (kürzen) um subtrees zu erhalten | Methode: cost complexity pruning (alpha zur Minimierung des durchschnittlichen Fehlers)\n",
    "\n",
    "### Classification Trees\n",
    "\n",
    "Gini-index um node Reinheit zu bestimmen: je niedriger, umso mehr/wahrscheinlicher sind die Ergebnisse aus derselben Klasse\n",
    "Entropie: Ähnlich zu Gini-index, bestimmt ebenfalls Reinheit\n",
    "classification error rate: genutzt wenn accuracy des finalen, gekürtzten trees das Ziel ist.\n",
    "\n",
    "Vorteie von Trees: \n",
    "- einfacher zu verstehen und zu erklären\n",
    "- bilden menschliche Entscheidungsfindung besser ab als klassische Reg und KLa\n",
    "- Können graphisch dargestellt und einfach verstanden werden\n",
    "\n",
    "Nachteile von Trees:\n",
    "- weniger akkurat\n",
    "- können weniger robust sein\n",
    "\n",
    "### Bagging\n",
    "\n",
    "separate Trainingssets schaffen, Mittel daraus bilden -> reduziert Varianz, genauerer Tree\n",
    "große Zahl separater Trainingssets führt nicht zu overfitting!\n",
    "Bäume korrelieren miteinander, da starke Predictors immer zuerst gesplittet werden\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "keine Korrelatin mehr, bessere performance\n",
    "-> Bedeutungsgewichtung von Predictors macht einen Unterschied!\n",
    "\n",
    "### Boosting\n",
    "\n",
    "lernt langsam, indem Baum an Reste anpasst.\n",
    "Baut Bäume aufeinander auf\n",
    "Kann overfitten wenn zu viele Trainingssets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c4a69",
   "metadata": {},
   "source": [
    "## Prüfungsfragen\n",
    "\n",
    "Hyperparameteranpassung: Gridsearch, Kombinationscheck \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe505a7",
   "metadata": {},
   "source": [
    "# Python Notizen\n",
    "\n",
    "source activate xgboost\n",
    "```python \n",
    "#virtuelle Umgebung starten (hier xgboost)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
